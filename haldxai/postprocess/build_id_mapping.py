# haldxai/postprocess/build_id_mapping.py
# ============================================================
"""HALD ç»Ÿä¸€å®ä½“ ID æ˜ å°„ç”Ÿæˆå™¨
--------------------------------------------------------------
è¾“å…¥
  â€¢ data/finals/collected_ext_nodes.csv
  â€¢ data/finals/collected_ext_relations.csv
  â€¢ data/finals/all_annotated_entities.csv
  â€¢ data/finals/all_annotated_relationships.csv
  â€¢ config/node_source_config.json            # ç”¨äºè§£æåŒä¹‰è¯åˆ—
è¾“å‡º
  â€¢ cache/name2id.csv / .json
  â€¢ cache/id2name.csv  / .json
ç”¨æ³•
  # ç›´æ¥å‡½æ•°è°ƒç”¨
from haldxai.postprocess.build_id_mapping import build_id_mapping
build_id_mapping("F:/Project/HALDxAI-Suite/HALDxAI-Project", force=True)

  # CLI
  python -m haldxai.postprocess.build_id_mapping run \
         --root F:/Project/HALDxAI-Suite/HALDxAI-Project --force
"""
from __future__ import annotations
import json, re, hashlib, csv, logging
from pathlib import Path
from collections import defaultdict, Counter
from typing import Any, Dict, List

import pandas as pd
import typer, rich

# ---------- æ—¥å¿— ----------
logging.basicConfig(level=logging.INFO,
                    format="%(levelname)s: %(message)s")
log = logging.getLogger("id-map")

# ---------- é€šç”¨ & æ­£åˆ™ ----------
PAT_SYN = re.compile(r'(gene_alias(_description)?$|synonyms$|external_synonyms$|entity_synonyms$)', re.I)
canonical = lambda s: str(s).strip().upper()
make_node_id = lambda text: "Entity-" + hashlib.md5(text.encode("utf-8")).hexdigest()[:10]

# ---------- è¯»å–å·¥å…· ----------
def _load_csv(fp: Path, **kw) -> pd.DataFrame:
    """å°è¯•ä¸¤ç§å¼•å·æ–¹æ¡ˆï¼Œå°½é‡é²æ£’"""
    for qchar in ('"', "'"):
        try:
            return pd.read_csv(fp, dtype=str, low_memory=False,
                               quoting=csv.QUOTE_MINIMAL, quotechar=qchar, **kw)
        except Exception:
            continue
    # å†æ¬¡ fallback
    return pd.read_csv(fp, dtype=str, low_memory=False, **kw)

def _load_parquet(fp: Path) -> pd.DataFrame:
    return pd.read_parquet(fp) if fp.exists() else pd.DataFrame()

# ========================== æ ¸å¿ƒ ========================== #
def build_id_mapping(project_root: str | Path, *, force: bool = False) -> None:
    root      = Path(project_root)
    finals_dir    = root / "data/finals"
    ext_dir   = root / "data/external_db"
    cfg_dir   = root / "configs"
    cache_dir = root / "cache"
    mappings_dir = root / "data/mappings"
    mappings_dir.mkdir(exist_ok=True, parents=True)

    out_name2id = mappings_dir / "name2id.csv"
    out_id2name = mappings_dir / "id2name.csv"

    if not force and out_name2id.exists() and out_id2name.exists():
        log.warning("æ˜ å°„å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚è‹¥éœ€é‡å»ºè¯·åŠ  --force")
        return

    # ---------- 1. åŒä¹‰è¯å­—å…¸ï¼ˆæ¥è‡ª node_source_configï¼‰ ----------
    syn_rows: List[Dict[str, str]] = []
    node_cfg: List[Dict[str, Any]] = json.loads((cfg_dir / "node_source_config.json").read_text(encoding="utf-8"))

    for cfg in node_cfg:
        fp      = ext_dir / cfg["file"]
        key_col = cfg["key_col"]
        df      = _load_csv(fp)

        # åŒä¹‰è¯åˆ—
        syn_cols = [c for c in df.columns if PAT_SYN.search(c)]
        if not syn_cols:
            continue

        for _, row in df.iterrows():
            primary = str(row.get(key_col, "")).strip()
            if not primary:                 # æ— ä¸»åè·³è¿‡
                continue

            syn_set = {primary}
            for sc in syn_cols:
                raw = str(row.get(sc, "")).strip()
                if raw and raw.lower() not in {"nan", "none"}:
                    parts = re.split(r'[;|]', raw)
                    syn_set.update(p.strip() for p in parts if p.strip())

            for syn in syn_set:
                syn_rows.append({
                    "synonym"      : syn,
                    "primary_name" : primary,
                    "source_table" : cfg["file"]
                })

    df_syn = (pd.DataFrame(syn_rows)
                .drop_duplicates()
                .assign(canonical_primary=lambda d: d["primary_name"].map(canonical)))

    # ---------- 2. å†å² â€œæ‰€æœ‰å‡ºç°è¿‡çš„å®ä½“å†™æ³•â€ ----------
    def _safe_read(name):  # helper
        fp = mappings_dir / name
        return _load_csv(fp) if fp.exists() else pd.DataFrame()

    df_ext_nodes = _load_parquet(cache_dir / "collected_ext_nodes_clean_entity_string.parquet")
    df_ext_rels = _load_parquet(cache_dir / "collected_ext_relations_clean_entity_string.parquet")
    df_llm_ents = _load_parquet(cache_dir / "all_annotated_entities_clean_entity_string.parquet")
    df_llm_rels = _load_parquet(cache_dir / "all_annotated_relationships_clean_entity_string.parquet")

    # 2) å¦‚æœ cache é‡Œä¸ºç©ºï¼Œå† fallback åˆ° finals/csvï¼ˆå¯é€‰ï¼‰
    def _fallback_csv(name: str) -> pd.DataFrame:
        fp = finals_dir / name
        return _load_csv(fp) if fp.exists() else pd.DataFrame()

    if df_ext_nodes.empty: df_ext_nodes = _fallback_csv("collected_ext_nodes.csv")
    if df_ext_rels.empty:  df_ext_rels = _fallback_csv("collected_ext_relations.csv")
    if df_llm_ents.empty:  df_llm_ents = _fallback_csv("all_annotated_entities.csv")
    if df_llm_rels.empty:  df_llm_rels = _fallback_csv("all_annotated_relationships.csv")

    all_terms = pd.concat([
        df_ext_nodes.get("entity_name", pd.Series(dtype=str)),
        df_ext_rels.get("source_name", pd.Series(dtype=str)),
        df_ext_rels.get("target_name", pd.Series(dtype=str)),
        df_llm_ents.get("main_text", pd.Series(dtype=str)),
        df_llm_rels.get("source_main_text", pd.Series(dtype=str)),
        df_llm_rels.get("target_main_text", pd.Series(dtype=str)),
    ], ignore_index=True).dropna()

    # ---------- 3. åˆ†é… ID ----------
    canonical_to_id: Dict[str, str] = {
        cp: make_node_id(cp) for cp in df_syn["canonical_primary"].unique()
    }
    HALD_NAME2ID: Dict[str, str] = {}
    HALD_ID2NAME: Dict[str, str] = {}

    # 3-a åŒä¹‰è¯å…ˆå…¥
    for syn, prim, can in df_syn[["synonym", "primary_name", "canonical_primary"]].itertuples(index=False):
        eid = canonical_to_id[can]
        HALD_NAME2ID[syn] = eid
        HALD_ID2NAME.setdefault(eid, prim)   # ä¸»åä¼˜å…ˆ

    # 3-b å†å²å†™æ³•
    name_counter: Dict[str, Counter] = defaultdict(Counter)
    for t in all_terms:
        t = str(t).strip()
        if not t:
            continue
        c = canonical(t)
        name_counter[c][t] += 1

    for can, cnt in name_counter.items():
        eid = canonical_to_id.setdefault(can, make_node_id(can))
        # å†™æ³•æ˜ å°„
        for variant in cnt:
            HALD_NAME2ID.setdefault(variant, eid)
        # è‹¥è¿˜æ²¡æœ‰ä¼˜é€‰å†™æ³• â†’ é€‰å‡ºç°é¢‘æ¬¡æœ€é«˜çš„
        HALD_ID2NAME.setdefault(eid, cnt.most_common(1)[0][0])

    log.info(f"âœ“ NAMEâ†’ID æ˜ å°„æ•°: {len(HALD_NAME2ID):,}")
    log.info(f"âœ“ IDâ†’NAME æ˜ å°„æ•°: {len(HALD_ID2NAME):,}")

    # ---------- 4. è¾“å‡º ----------
    pd.DataFrame(HALD_NAME2ID.items(), columns=["name", "entity_id"])\
      .to_csv(out_name2id, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL)
    pd.DataFrame(HALD_ID2NAME.items(),  columns=["entity_id", "best_name"])\
      .to_csv(out_id2name, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL)

    # é¢å¤– JSONï¼ˆå¯é€‰ï¼‰
    (mappings_dir / "name2id.json").write_text(json.dumps(HALD_NAME2ID, ensure_ascii=False, indent=2), encoding="utf-8")
    (mappings_dir / "id2name.json").write_text(json.dumps(HALD_ID2NAME, ensure_ascii=False, indent=2),  encoding="utf-8")

    rich.print(f"[bold green]ğŸ‰ ID æ˜ å°„å·²ç”Ÿæˆ[/]\n"
               f"  â€¢ {out_name2id}\n  â€¢ {out_id2name}")

# ========================== Typer CLI ========================== #
app = typer.Typer(pretty_exceptions_show_locals=False)

@app.command("run")
def _run(root: str = typer.Option(..., help="é¡¹ç›®æ ¹ç›®å½•"),
         force: bool = typer.Option(False, "--force/-f", help="è¦†ç›–å·²æœ‰è¾“å‡º")):
    """ç”Ÿæˆ HALD ç»Ÿä¸€å®ä½“ ID æ˜ å°„"""
    build_id_mapping(root, force=force)

if __name__ == "__main__":
    app()
