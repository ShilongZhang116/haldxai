from __future__ import annotations

import re
from pathlib import Path
import pandas as pd

from haldxai.enrich.tables.loader import load_name2id, save_name2id
from haldxai.enrich.tables.utils import alloc_id


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è¾…åŠ©å‡½æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _clean_source(src: str) -> str:
    """æŠŠå¤–éƒ¨æ ‡å‡†åŒ– csv åå­—ç¼©çŸ­ä¸º ageanno__... / hagr__..."""
    src = re.sub(r"^hald_", "", src)
    src = re.sub(r"__std\.csv$", "", src)
    return src


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ ¸å¿ƒå…¥å£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_entity_types(
        project_root: Path,
        df_ext_nodes: pd.DataFrame,
        df_llm_entities: pd.DataFrame,
        df_pred_entities: pd.DataFrame,
        *,
        force: bool = False
) -> pd.DataFrame:

    db_dir = project_root / "data/database"
    db_dir.mkdir(parents=True, exist_ok=True)

    output_csv = db_dir / "entity_types.csv"

    if output_csv.exists() and not force:
        print(f"ğŸŸ¡ entity_types.csv å·²å­˜åœ¨ï¼ˆè·³è¿‡ï¼‰ã€‚pass `force=True` ä»¥é‡æ–°ç”Ÿæˆã€‚")
        return pd.read_csv(output_csv)

    print("â–¶ æ„å»º entity_types.csv â€¦")

    # â”€â”€ 0) æ˜ å°„åŠ è½½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    name2id = load_name2id(project_root)

    # ---------- 1. å–å„æ¥æºå¹¶ç»Ÿä¸€åˆ— ----------
    ext_nodes = (
        df_ext_nodes[["entity_name", "entity_type", "source_file"]]
        .rename(columns={"source_file": "source"})
    )

    llm_ents = (
        df_llm_entities[["main_text", "entity_type", "model_name"]]
        .rename(columns={"main_text": "entity_name", "model_name": "source"})
    )

    bert_pred = (
        df_pred_entities[["main_text", "predicted_type"]]
        .rename(columns={"main_text": "entity_name", "predicted_type": "entity_type"})
    )

    bert_pred["source"] = "bert_model_prediction"

    df_all = pd.concat([ext_nodes, llm_ents, bert_pred], ignore_index=True)
    df_all = df_all.dropna(subset=["source"])

    # ---------- 2. è§„èŒƒåŒ– source ----------
    mask_ext = df_all["source"].str.startswith("hald_")  # åªæ¸…æ´—å¤–éƒ¨ csv
    df_all.loc[mask_ext, "source"] = df_all.loc[mask_ext, "source"].map(_clean_source)

    # ---------- 3. æ˜ å°„ entity_id ----------
    df_all["entity_id"] = df_all["entity_name"].apply(lambda n: alloc_id(name2id, n))

    # ---------- 4. å»é‡ ----------
    df_all = (
        df_all.drop_duplicates(subset=["entity_id", "entity_type", "source"])
        .reset_index(drop=True)
    )

    # ---------- 5. è‡ªå¢ä¸»é”® ----------
    df_all.insert(0, "etype_pk", range(1, len(df_all) + 1))

    # ------------- 5. ä¿å­˜ ------------------
    df_all.to_csv(output_csv, index=False, encoding="utf-8-sig")
    print(f"âœ“ entity_types å†™å‡º {len(df_all):,} è¡Œ â†’ {output_csv}")

    save_name2id(project_root, name2id)  # â‘¡ æŠŠå¯èƒ½æ–°å¢çš„æ˜ å°„è½ç›˜
    print("âœ“ name2id.json å·²æ›´æ–°ï¼Œå½“å‰æ¡æ•° =", len(name2id))

    return df_all