# haldxai/tables/entity_catalog.py
from __future__ import annotations
from pathlib import Path
import json, hashlib, pandas as pd

from haldxai.enrich.tables.loader import load_name2id, save_name2id
from haldxai.enrich.tables.utils import alloc_id

def _pg_array(elems: list[str]) -> str:
    if not elems:
        return "{}"

    def esc(s: str) -> str:
        # 1) å…ˆå¤„ç†åæ–œæ 
        s = s.replace("\\", "\\\\")
        # 2) å†å¤„ç†åŒå¼•å·
        s = s.replace('"', '\\"')
        # 3) å¤–å±‚åŠ å¼•å·
        return f'"{s}"'
    return "{" + ",".join(esc(e) for e in elems) + "}"

def build_entity_catalog(
        project_root: Path,
        *,
        force: bool = False
) -> pd.DataFrame:
    """
    æ ¹æ® LLM æ³¨é‡Šå®ä½“ + BioPortal ç»“æœï¼Œæ±‡æ€»æˆ entity_catalog
    """

    db_dir      = project_root / "data/database"
    db_dir.mkdir(parents=True, exist_ok=True)

    output_csv = db_dir / "entity_catalog.csv"

    if output_csv.exists() and not force:
        print(f"ğŸŸ¡ entity_catalog.csv å·²å­˜åœ¨ï¼ˆè·³è¿‡ï¼‰ã€‚pass `force=True` ä»¥é‡æ–°ç”Ÿæˆã€‚")
        return pd.read_csv(output_csv)

    print("â–¶ æ„å»º entity_catalog.csv â€¦")

    # ------------------------------------------------------------------ #
    # 1. è¯»å– name2id æ˜ å°„
    # ------------------------------------------------------------------ #
    name2id = load_name2id(project_root)

    # ------------------------------------------------------------------ #
    # 2. è§£æ BioPortal info
    # ------------------------------------------------------------------ #
    bp_json = project_root / "data/ner_dict/bioPortal/final_entity_results.json"
    bp_map  = json.loads(bp_json.read_text(encoding="utf-8"))

    rows: list[dict] = []
    for name, info in bp_map.items():
        items = info.get("search_result", {}).get("items", [])
        for it in items:
            rows.append({
                "entity_name":   name,
                "pref_label":    it.get("pref_label"),
                "definitions":   (it.get("definitions") or [None])[0],
                "synonyms":      it.get("synonyms") or [],
                "ontology":      it.get("ontology"),
                "class_iri":     it.get("class_iri"),
            })

    df = pd.DataFrame(rows)
    if df.empty:
        print("âš  æ— æœ‰æ•ˆ BioPortal è®°å½•ï¼Œè¿”å›ç©ºè¡¨")
        return df

    # ------------------------------------------------------------------ #
    # 3. è®¡ç®— entity_id
    # ------------------------------------------------------------------ #
    df["entity_id"] = df["entity_name"].apply(lambda n: alloc_id(name2id, n))

    # ------------------------------------------------------------------ #
    # 4. å­—æ®µèšåˆ
    # ------------------------------------------------------------------ #
    clean = lambda seq: [s for s in seq if isinstance(s, str) and s.strip()]

    out = (
        df.groupby("entity_id")
        .agg(
            entity_name=("entity_name", "first"),
            pref_label=("pref_label", lambda x: ";".join(sorted(clean(x)))),
            definitions=("definitions", lambda x: ";".join(sorted(clean(x)))),
            synonyms=("synonyms",
                      lambda col: _pg_array(
                          sorted(clean(
                              y for lst in col if isinstance(lst, list)
                              for y in lst
                          ))
                      )),
            ontology=("ontology", lambda x: ";".join(sorted(clean(x)))),
            class_iri=("class_iri", lambda x: ";".join(sorted(clean(x))))
        )
        .reset_index()
    )

    # å¢åŠ ä¸»é”®
    out.insert(0, "pk", range(1, len(out) + 1))

    save_name2id(project_root, name2id)  # â‘¡ æŠŠå¯èƒ½æ–°å¢çš„æ˜ å°„è½ç›˜
    print("âœ“ name2id.json å·²æ›´æ–°ï¼Œå½“å‰æ¡æ•° =", len(name2id))

    out.to_csv(output_csv, index=False, encoding="utf-8-sig")
    print(f"âœ“ entity_catalog å†™å‡º {len(out):,} è¡Œ â†’ {output_csv}")

    return out