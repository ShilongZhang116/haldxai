#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
build_unified_graph.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. è¯»å– cache ä¸­æ¸…æ´—åçš„ parquetï¼š
   Â· collected_ext_nodes_clean.parquet
   Â· collected_ext_rels_clean.parquet
   Â· annotated_entities_clean.parquet
   Â· annotated_relationships_clean.parquet
2. åˆå¹¶æˆ
   Â· all_nodes.parquet      ï¼ˆå»é‡å¹¶èšåˆ entity_type / sourceï¼‰
   Â· all_rels.parquet       ï¼ˆå»é‡ï¼‰
3. ä¾æ® mappings/name2id.json è¿½åŠ  entity_id / relation_id
4. è¾“å‡º
   Â· all_nodes_with_id.parquet
   Â· all_rels_with_id.parquet
ç”¨æ³•ï¼ˆTyper CLIï¼‰:
$ python -m haldxai.enrich.graph_build.build_unified_graph run --root F:/Project/HALDxAI-Suite/HALDxAI-Project
"""
from __future__ import annotations

import json, logging, pandas as pd
from pathlib import Path
import typer

logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ ¸å¿ƒå‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def build_unified_graph(project_root: Path, *, force: bool = False) -> None:
    cache_dir   = project_root / "cache"
    map_dir     = project_root / "data/mappings"
    map_file    = map_dir / "name2id.json"

    # ---------- è¾“å…¥ ----------
    f_ext_nodes = cache_dir / "collected_ext_nodes_clean.parquet"
    f_ext_rels  = cache_dir / "collected_ext_rels_clean.parquet"
    f_ann_ents  = cache_dir / "annotated_entities_clean.parquet"
    f_ann_rels  = cache_dir / "annotated_relationships_clean.parquet"

    f_all_nodes = cache_dir / "all_nodes_with_id.parquet"
    f_all_rels  = cache_dir / "all_rels_with_id.parquet"

    if not force and f_all_nodes.exists() and f_all_rels.exists():
        logger.warning("å·²å­˜åœ¨ all_nodes_with_id / all_rels_with_idï¼Œè·³è¿‡ã€‚"
                       "å¦‚éœ€è¦†ç›–è¯·åŠ  --force")
        return

    logger.info("â–¶ è¯»å– parquet â€¦")
    ext_nodes = pd.read_parquet(f_ext_nodes)
    ext_rels  = pd.read_parquet(f_ext_rels)
    ann_ents  = pd.read_parquet(f_ann_ents)
    ann_rels  = pd.read_parquet(f_ann_rels)

    # ---------- åˆå¹¶èŠ‚ç‚¹ ----------
    logger.info("â–¶ åˆå¹¶èŠ‚ç‚¹ â€¦")
    ext_nodes_std = (
        ext_nodes.rename(columns={"entity_name": "entity_name",
                                  "entity_type": "entity_type",
                                  "source_file": "source"})
                  .loc[:, ["entity_name", "entity_type", "source"]]
    )
    ann_nodes_std = (
        ann_ents.rename(columns={"main_text": "entity_name",
                                 "entity_type": "entity_type"})
                 .loc[:, ["entity_name", "entity_type"]]
    )
    ann_nodes_std["source"] = "pubmed_article_llm"

    nodes_merged = pd.concat([ext_nodes_std, ann_nodes_std], ignore_index=True)

    all_nodes = (
        nodes_merged
        .groupby("entity_name", as_index=False)
        .agg({
            "entity_type": lambda x: ";".join(sorted(set(x.dropna()))),
            "source"     : lambda x: ";".join(sorted(set(x.dropna())))
        })
    )

    # ---------- åˆå¹¶å…³ç³» ----------
    logger.info("â–¶ åˆå¹¶å…³ç³» â€¦")
    ext_rels_std = (
        ext_rels.rename(columns={"source_name": "source_entity_name",
                                 "target_name": "target_entity_name",
                                 "relation_type": "relation_type",
                                 "source_file": "source"})
                 .loc[:, ["source_entity_name", "target_entity_name",
                          "relation_type", "source"]]
    )
    ann_rels_std = (
        ann_rels.rename(columns={"source_main_text": "source_entity_name",
                                 "target_main_text": "target_entity_name",
                                 "relation_type": "relation_type"})
                 .loc[:, ["source_entity_name", "target_entity_name",
                          "relation_type"]]
    )
    ann_rels_std["source"] = "pubmed_article_llm"

    all_rels = (
        pd.concat([ext_rels_std, ann_rels_std], ignore_index=True)
          .drop_duplicates(subset=["source_entity_name",
                                   "target_entity_name",
                                   "relation_type"])
    )

    # ---------- åŠ è½½ name2id ----------
    with map_file.open(encoding="utf-8") as fh:
        name2id: dict[str, str] = json.load(fh)

    def map_name(n: str) -> str | pd.NA:
        return name2id.get(n, pd.NA)

    # ---------- èŠ‚ç‚¹åŠ  ID ----------
    all_nodes["entity_id"] = all_nodes["entity_name"].map(map_name)

    # ---------- å…³ç³»åŠ  ID & ç”Ÿæˆ relation_id ----------
    all_rels["source_entity_id"] = all_rels["source_entity_name"].map(map_name)
    all_rels["target_entity_id"] = all_rels["target_entity_name"].map(map_name)

    all_rels["relation_id"] = (
        "Relation-" +
        all_rels["source_entity_id"].str.replace("^Entity-", "", regex=True).astype(str) +
        "-" +
        all_rels["target_entity_id"].str.replace("^Entity-", "", regex=True).astype(str)
    )

    # ---------- åˆ—é¡ºåº ----------
    all_nodes = all_nodes[["entity_id", "entity_name", "entity_type", "source"]]
    all_rels  = all_rels[["relation_id", "source_entity_name", "target_entity_name",
                          "relation_type", "source",
                          "source_entity_id", "target_entity_id"]]

    # ---------- ä¿å­˜ ----------
    all_nodes.to_parquet(f_all_nodes, index=False)
    all_rels.to_parquet(f_all_rels,   index=False)

    logger.info("ğŸ‰ å®Œæˆå†™å‡ºï¼š\n"
                f"    â€¢ {f_all_nodes}  ({len(all_nodes):,})\n"
                f"    â€¢ {f_all_rels}   ({len(all_rels):,})")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Typer CLI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
app = typer.Typer(pretty_exceptions_show_locals=False)

@app.command("run")
def _run(root: str = typer.Option(..., help="é¡¹ç›®æ ¹ç›®å½•"),
         force: bool = typer.Option(False, "--force", "-f", help="è¦†ç›–å·²å­˜åœ¨è¾“å‡º")):
    logging.basicConfig(format="%(levelname)s: %(message)s", level=logging.INFO)
    build_unified_graph(Path(root), force=force)

if __name__ == "__main__":
    app()
