# haldxai/pubmed/fetch.py
import io
import os
import time
import yaml
import pandas as pd
from Bio import Entrez, Medline
from urllib.error import HTTPError, URLError
from haldxai.pubmed.process import process_record, merge_results


def fetch_pubmed_data(query, email, summary_file, api_key=None, retmax=None, batch_size=500):
    """
    ä¸»å‡½æ•°ï¼šä½¿ç”¨ Entrez API è·å– PubMed æ–‡çŒ®æ•°æ®å¹¶å­˜ä¸º csvï¼Œå¸¦ checkpointã€‚
    """
    Entrez.email = email
    if api_key:
        Entrez.api_key = api_key

    retmax = retmax or 100000

    search_handle = Entrez.esearch(
        db="pubmed", term=query, usehistory="y", retmax=retmax
    )
    search_result = Entrez.read(search_handle)
    search_handle.close()

    webenv = search_result["WebEnv"]
    query_key = search_result["QueryKey"]
    total_count = int(search_result["Count"])

    if retmax > total_count:
        print(f"ğŸ” æ‰¾åˆ° {total_count} ç¯‡æ–‡çŒ®ï¼Œè®¡åˆ’è·å– {total_count} ç¯‡")
    else:
        print(f"ğŸ” æ‰¾åˆ° {total_count} ç¯‡æ–‡çŒ®ï¼Œè®¡åˆ’è·å– {retmax} ç¯‡")

    downloaded_pmids = set()
    checkpoint_df = pd.DataFrame()

    if os.path.exists(summary_file):
        try:
            checkpoint_df = pd.read_csv(summary_file, dtype=str)
            downloaded_pmids = set(str(int(float(pmid))) for pmid in checkpoint_df["pmid"].dropna())
            print(f"âœ… åŠ è½½ Checkpointï¼Œå·²ä¸‹è½½ {len(downloaded_pmids)} ç¯‡")
        except Exception as e:
            print(f"âš ï¸ Checkpoint è¯»å–å¤±è´¥: {e}")

    all_pmids = set(str(pmid) for pmid in search_result["IdList"])
    remaining_pmids = list(all_pmids - downloaded_pmids)


    if not remaining_pmids:
        print("ğŸ‰ æ‰€æœ‰æ–‡çŒ®å·²ä¸‹è½½")
        return checkpoint_df

    new_results = []
    for i in range(0, len(remaining_pmids), batch_size):
        batch_pmids = remaining_pmids[i:i+batch_size]
        try:
            fetch_handle = Entrez.efetch(
                db="pubmed", id=",".join(batch_pmids),
                rettype="medline", retmode="text"
            )
            data = fetch_handle.read()
            fetch_handle.close()

            batch = [process_record(rec) for rec in Medline.parse(io.StringIO(data))]
            df_batch = pd.DataFrame(batch)
            df_batch.to_csv(summary_file, mode='a', header=not os.path.exists(summary_file),
                            index=False, encoding='utf-8-sig')
            new_results.extend(batch)
            print(f"âœ… è·å– {i+1}-{i+len(batch)} æ¡")

            time.sleep(1)
        except Exception as e:
            print(f"â›” é”™è¯¯ï¼š{e}")
            continue

    return merge_results(checkpoint_df, new_results)


def generate_query_with_time(query, start_date, end_date):
    return f"({query}) AND ({start_date}[Date - Publication] : {end_date}[Date - Publication])"
