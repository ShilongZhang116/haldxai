from pathlib import Path
import re
import os
import spacy
import subprocess
import sys
from spacy.cli import download as spacy_download
import nltk
from nltk.tokenize import sent_tokenize

# ---------- è·¯å¾„ & å¹´ä»½ ---------- #
def detect_years(path: Path, prefix: str) -> list[int]:
    years = set()
    for p in path.glob(f"{prefix}_Y*.csv"):
        m = re.search(r"_Y(\d{4})\.csv$", p.name)
        if m:
            years.add(int(m.group(1)))
    return sorted(years)

def build_save_path(base: Path, model: str, year: int, ext: str = "csv") -> Path:
    out_dir = base / model
    out_dir.mkdir(parents=True, exist_ok=True)
    return out_dir / f"ner_{year}.{ext}"

# ---------- SpaCy æ¨¡å‹ ---------- #
def ensure_spacy_model(model_name: str, local_repo: Path | str = "models/SciSpacy"):

    # â‘  å·²å®‰è£…ï¼Ÿ
    try:
        return spacy.load(model_name)
    except (OSError, IOError):
        pass                      # â†’ è¿›å…¥â‘¡

    # â‘¡ æœ¬åœ° tar.gzï¼Ÿ
    local_repo = Path(local_repo)
    if local_repo.exists():
        pattern = f"{model_name}*tar.gz"
        candidates = list(local_repo.glob(pattern))
        if candidates:
            pkg_path = candidates[0]
            print(f"ğŸ“¦ ä»æœ¬åœ°å®‰è£… {pkg_path.name} â€¦")
            try:
                subprocess.check_call(
                    [sys.executable, "-m", "pip", "install", pkg_path]
                )
                return spacy.load(model_name)
            except Exception as e:
                print(f"âš ï¸ æœ¬åœ°å®‰è£…å¤±è´¥ï¼š{e}")

    # â‘¢ åœ¨çº¿ä¸‹è½½
    print(f"ğŸŒ æœ¬åœ°æ— æ¨¡å‹ {model_name}ï¼Œæ­£åœ¨åœ¨çº¿ä¸‹è½½ â€¦")
    spacy_download(model_name)
    return spacy.load(model_name)

# ---------- NLTKæ–­å¥ ---------- #
def get_sentences_containing_offsets(text: str, offsets: list[tuple[int, int]]) -> str:
    """
    æå–åŒ…å«æ‰€æœ‰ offsets çš„æœ€å°å¥å­ç»„åˆ

    Args:
        text:     åŸå§‹æ–‡æœ¬
        offsets:  [(start, end), ...]  â€” å®ä½“åœ¨ text ä¸­çš„ç´¢å¼•åŒºé—´

    Returns:
        str: ç”±ä¸€ä¸ªæˆ–å¤šä¸ªå¥å­æ‹¼æ¥è€Œæˆï¼Œä¿è¯è¦†ç›–æ‰€æœ‰å®ä½“
    """
    # ç¡®ä¿ punkt åˆ†è¯æ¨¡å‹å·²å°±ç»ª
    try:
        _ = nltk.data.find("tokenizers/punkt")
    except LookupError:  # é¦–æ¬¡ä½¿ç”¨è‡ªåŠ¨ä¸‹è½½
        nltk.download("punkt", quiet=True)

    # å¥å­åˆ‡åˆ†åŠå…¶ span
    sentences = sent_tokenize(text)
    spans = list(nltk.tokenize.PunktSentenceTokenizer().span_tokenize(text))

    collected = []
    for s_start, s_end in spans:
        # å½“å‰å¥å­æ˜¯å¦åŒ…å«ä»»ä½•å®ä½“
        if any(s_start <= ent_start and ent_end <= s_end for ent_start, ent_end in offsets):
            collected.append(text[s_start:s_end].strip())

    if collected:
        return " ".join(collected)

    # fallback: è¿”å›è·ç¦»ç¬¬ä¸€ä¸ªå®ä½“æœ€è¿‘çš„å¥å­
    first_ent_start = offsets[0][0]
    nearest_span = min(spans, key=lambda sp: abs(first_ent_start - sp[0]))
    return text[nearest_span[0]:nearest_span[1]].strip()